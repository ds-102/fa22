{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Initialize Otter\n",
    "import otter\n",
    "grader = otter.Notebook(\"lab10.ipynb\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 10: Reinforcement Learning with Data\n",
    "Welcome to the 10th DS102 lab! \n",
    "\n",
    "The goal of this lab is to explore solving MDPs by collecting data. This will include simple Monte Carlo estimates from offline data, and the online Q-learning algorithm. \n",
    "\n",
    "The code you need to write is indicated with `...`. There is additional documentation for each part as you go along.\n",
    "\n",
    "## Collaboration Policy\n",
    "Data science is a collaborative activity. While you may talk with others about the labs, we ask that you **write your solutions individually**. If you do discuss the assignments with others please **include their names** in the cell below.\n",
    "\n",
    "## Gradescope Submission\n",
    "\n",
    "To submit the lab: \n",
    "1. Navigate to Kernel > Restart & Run All. Ensure that all public test cases pass locally. \n",
    "2. Save your notebook under File > Save and Checkpoint. If you do not save your notebook, then you might run into issues with the downloaded .zip file.\n",
    "3. Run the very last cell, which generates a .zip file for you to download to your local machine. Click the “here” button to download the .zip file. You may receive an error that the .zip file was unable to be created because there was an issue with PDF generation. You need to ensure that you’ve answered all of the questions that require a manual response.\n",
    "4. If your work downloads as several independent files rather than as a .zip, you are likely using Safari on a Mac. Follow these instructions to make sure you can download your work as a zip: https://macreports.com/how-to-download-zip-files-without-unzipping/\n",
    "5. Upload this .zip to the correct assignment on Gradescope. After submitting, the autograder built-in to the Gradescope assignment will tell you which public test cases you’ve passed and failed. There are no hidden test cases.\n",
    "\n",
    "**For full credit, this assignment should be completed and submitted before Monday, November 21, 2022 at 11:59 PM. PST**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collaborators\n",
    "Write the names of your collaborators in this cell.\n",
    "\n",
    "`<Collaborator Name> <Collaborator e-mail>`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import sys\n",
    "import time\n",
    "\n",
    "import hashlib\n",
    "def get_hash(num, significance = 4):\n",
    "    num = round(num, significance)\n",
    "    \"\"\"Helper function for assessing correctness\"\"\"\n",
    "    return hashlib.md5(str(num).encode()).hexdigest()\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the GridWorld class \n",
    "We begin by defining a class for the environment in which we will run our experiments. This is the grid world from class where we can have both a stochastic or deterministic environment. In the stochastic case the agent will have a probability of 0.8 of going in the direction it's told to go, and a probability of 0.1 of going in each direction orthogonal to the direction it's meant to go in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "FORWARD_PROB = 0.8\n",
    "LEFT_PROB = 0.1\n",
    "RIGHT_PROB = 0.1\n",
    "BACKWARD_PROB = 0.0\n",
    "FIXED_PROB = 0.0\n",
    "class GridWorld():\n",
    "    \"\"\"The grid world class.\n",
    "    \n",
    "    Inputs:\n",
    "        grid : list of list of str\n",
    "            The starting representation of the world. A single element\n",
    "            must be \"R\" which represents the starting location of the robot.\n",
    "            Any element that is \"\" represents a cell on which the robot can travel,\n",
    "            \"X\" represents a rock which the robot can not travel on, and any\n",
    "            cell with a string that can be converted to a number represents\n",
    "            a terminal state with its corresponding reward.\n",
    "        stochastic : bool\n",
    "            Whether the environment is stochastic or deterministic.\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self, grid, stochastic):\n",
    "        self._grid = grid\n",
    "        self.num_rows = len(grid)\n",
    "        self.num_cols = len(grid[0])\n",
    "        self._stochastic = stochastic\n",
    "        # Determine the starting location of the robot.\n",
    "        for i in range(self.num_rows):\n",
    "            for j in range(self.num_cols):\n",
    "                if self._grid[i][j] == \"R\":\n",
    "                    self._grid[i][j] = \"\"\n",
    "                    self._row_pos = i\n",
    "                    self._col_pos = j\n",
    "                    self._start_row_pos = i\n",
    "                    self._start_col_pos = j\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"Reset the environment to its original state.\"\"\"\n",
    "        self._row_pos = self._start_row_pos\n",
    "        self._col_pos = self._start_col_pos\n",
    "        return (self._row_pos, self._col_pos)\n",
    "        \n",
    "    def step(self, action):\n",
    "        \"\"\"Move the robot a single step in the world.\n",
    "        \n",
    "        Inputs:\n",
    "            action : str\n",
    "                The desired direction to travel in. Can either be\n",
    "                \"north\", \"west\", \"east\", \"south\".\n",
    "            \n",
    "        Outputs:\n",
    "            pos : tuple of int\n",
    "                The location the robot ends up at after taking a step.\n",
    "                The first element represents the row and the second element\n",
    "                represents the column.\n",
    "            reward : float\n",
    "                The reward from taking this step.\n",
    "            done : bool\n",
    "                Whether the robot has reached a terminal state or not.\n",
    "\n",
    "        \"\"\"\n",
    "        # Determine the transition probabilities based on the action and\n",
    "        # whether the environment is stochastic or deterministic.\n",
    "        if self._stochastic:\n",
    "            if action == \"north\":\n",
    "                transition_probs = {\n",
    "                    \"north\": FORWARD_PROB,\n",
    "                    \"west\": LEFT_PROB,\n",
    "                    \"east\": RIGHT_PROB,\n",
    "                    \"south\": BACKWARD_PROB,\n",
    "                    \"fixed\": FIXED_PROB\n",
    "                }\n",
    "            if action == \"west\":\n",
    "                transition_probs = {\n",
    "                    \"north\": RIGHT_PROB,\n",
    "                    \"west\": FORWARD_PROB,\n",
    "                    \"east\": BACKWARD_PROB,\n",
    "                    \"south\": LEFT_PROB,\n",
    "                    \"fixed\": FIXED_PROB\n",
    "                }\n",
    "            if action == \"east\":\n",
    "                transition_probs = {\n",
    "                    \"north\": LEFT_PROB,\n",
    "                    \"west\": BACKWARD_PROB,\n",
    "                    \"east\": FORWARD_PROB,\n",
    "                    \"south\": RIGHT_PROB,\n",
    "                    \"fixed\": FIXED_PROB\n",
    "                }\n",
    "            if action == \"south\":\n",
    "                transition_probs = {\n",
    "                    \"north\": BACKWARD_PROB,\n",
    "                    \"west\":RIGHT_PROB,\n",
    "                    \"east\": LEFT_PROB,\n",
    "                    \"south\": FORWARD_PROB,\n",
    "                    \"fixed\": FIXED_PROB\n",
    "                }\n",
    "        else:\n",
    "            transition_probs = {\n",
    "                \"north\": 0.0,\n",
    "                \"west\": 0.0,\n",
    "                \"east\": 0.0,\n",
    "                \"south\": 0.0,\n",
    "                \"fixed\": 0.0\n",
    "            }\n",
    "            transition_probs[action] = 1.0\n",
    "            \n",
    "        # Account for the cases where we are on the boundaries or\n",
    "        # next to a rock.\n",
    "        row = self._row_pos\n",
    "        col = self._col_pos\n",
    "        if row == 0 or self._grid[row - 1][col] == \"X\":\n",
    "            transition_probs[\"fixed\"] += transition_probs[\"north\"]\n",
    "            transition_probs[\"north\"] = 0.0\n",
    "        if col == 0 or self._grid[row][col - 1] == \"X\":\n",
    "            transition_probs[\"fixed\"] += transition_probs[\"west\"]\n",
    "            transition_probs[\"west\"] = 0.0\n",
    "        if row == self.num_rows - 1 or self._grid[row + 1][col] == \"X\":\n",
    "            transition_probs[\"fixed\"] += transition_probs[\"south\"]\n",
    "            transition_probs[\"south\"] = 0.0\n",
    "        if col == self.num_cols - 1 or self._grid[row][col + 1] == \"X\":\n",
    "            transition_probs[\"fixed\"] += transition_probs[\"east\"]\n",
    "            transition_probs[\"east\"] = 0.0\n",
    "\n",
    "        # Decide which direction the robot will go.\n",
    "        directions = list(transition_probs.keys())\n",
    "        probs = list(transition_probs.values())\n",
    "        move = np.random.choice(directions, p=probs)\n",
    "        if move == \"north\":\n",
    "            self._row_pos -= 1\n",
    "        elif move == \"west\":\n",
    "            self._col_pos -= 1\n",
    "        elif move == \"east\":\n",
    "            self._col_pos += 1\n",
    "        elif move == \"south\":\n",
    "            self._row_pos += 1\n",
    "\n",
    "        # Check if we are on a final state and determine the reward.\n",
    "        if self._grid[self._row_pos][self._col_pos] != \"\":\n",
    "            reward = float(self._grid[self._row_pos][self._col_pos])\n",
    "            done = True\n",
    "        else:\n",
    "            reward = 0.0\n",
    "            done = False\n",
    "            \n",
    "        return (self._row_pos, self._col_pos), reward, done\n",
    "            \n",
    "    def render(self):\n",
    "        \"\"\"Print an ASCII visualization of the world.\"\"\"\n",
    "        for i, row in enumerate(self._grid):\n",
    "            row_strs = []\n",
    "            for j, elt in enumerate(row):\n",
    "                sys.stdout.write(\" -----\")\n",
    "                if i == self._row_pos and j == self._col_pos:\n",
    "                    elt = \"R\"\n",
    "                row_strs.append(elt.center(5))\n",
    "            sys.stdout.write(\"\\n\")\n",
    "            sys.stdout.write(\"|\" + \"|\".join(row_strs) + \"|\")\n",
    "            sys.stdout.write(\"\\n\")\n",
    "        for _ in range(self.num_cols):\n",
    "            sys.stdout.write(\" -----\")\n",
    "        sys.stdout.flush()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we show you how to initialize and render a simple GridWorld environment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we initialize the environment.\n",
    "simple_env = GridWorld([[\"R\", \"\", \"100\",  \"X\", ],\n",
    "                        [\"\", \"X\", \"X\", \"X\"],\n",
    "                        [\"\", \"\", \"\",  \"100\"]], stochastic=False)\n",
    "\n",
    "# Here we draw the initial state to the screen.\n",
    "simple_env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And how the robot moves in this environment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we take the \"east\" action and then draw the new state to the screen.\n",
    "next_state, reward, is_done = simple_env.step(\"east\")\n",
    "simple_env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we print out the information that the \"step\" function returns.\n",
    "print(next_state, reward, is_done)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 1: Monte Carlo Policy Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far in class and in discussion section, we have considered the case where we have a **known** MDP and we want to solve for the value function. We've used the recursive Bellman equation to do so. For example, we can solve for the value of following a deterministic policy $\\pi$ starting from a state $s$ by using the following defintion:\n",
    "\n",
    "$$ V^{\\pi}(s) = \\mathbb{E}_{s'} \\left[ R(s, \\pi(s) ,s') + \\gamma V^{\\pi}(s') \\right] = \\sum_{s'} P(s' | s, \\pi(s)) \\left( R(s, \\pi(s) ,s') + \\gamma V^{\\pi}(s') \\right) $$\n",
    "\n",
    "As long as we know $P$, $R$, and $\\pi$', we can apply value iteration to compute $V^{\\pi}(s)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this question, we will begin to explore how to do Reinforcement Learning when we **don't know** $P$ or $R$. Instead we will collect data using a fixed policy $\\pi$ and then estimate the value function directly. The simplest approach is called **Monte Carlo** policy evaluation. We collect many trajectories and then take a simple sample average. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At first this might sound confusing. How do we compute a sample average for $V^{\\pi}(s)$? The righthand side requires that we know $V^{\\pi}(s)$! First, we need to introduce an equivalent but simpler way of writing the value function. It turns out that the value function is equivalent to the **average discounted sum of rewards** from running the policy $\\pi$ starting in a state $s$. For an episode that lasts for $T$ time steps, we can write mathematically:\n",
    "\n",
    "$$ V^{\\pi}(s) = \\mathbb{E} \\left[ \\sum_{t=1}^T \\gamma^{t-1} R(s_t, a_t, s'_t)  \\right]$$,\n",
    "\n",
    "where $s_1 = s$, $a_t \\sim \\pi(s_t)$, and $s'_t \\sim P(\\cdot | s_t, a_t)$. Note that the expectation is taken with respect to both the randomness in $\\pi$ (if any) and the randomness in $P$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Solving this expectation mathematically using $P$, $R$, and $\\pi$ is very difficult - that's why we typically use the Bellman equation instead. But if we collect many trajectories from running $\\pi$ in the environment, then this equation is much simpler: we just take the sample average."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a concrete example, let's consider the simple uniform random policy. This policy ignores the state and chooses between the four actions uniformly at random."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_policy(s):\n",
    "    \"\"\"Return a random action.\"\"\"\n",
    "    a = np.random.choice([\"north\", \"south\", \"east\", \"west\"])\n",
    "    return a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "## Question 1a\n",
    "\n",
    "Now you will write some code which takes a policy (in the form of a function from states to actions just like the code above) and then runs that policy in a given GridWorld environment so that we can collect a dataset.\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q1a\n",
    "manual: false\n",
    "points: 1\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_policy(pi, env, render=False):\n",
    "    \n",
    "    \"\"\"Run an agent that follows a fixed policy\n",
    "    \n",
    "    Inputs:\n",
    "        pi : function\n",
    "            A function that takes a state and returns an action\n",
    "        env : GridWorld\n",
    "            The environment in which to run the agent.\n",
    "        render : bool\n",
    "            Whether to print the environment as it goes through each iteration.\n",
    "    \n",
    "    Returns:\n",
    "        samples: list of tuple of float\n",
    "            A list of (state, action, next state, reward) tuples from running\n",
    "                the policy in the environment\n",
    "    \"\"\"\n",
    "    \n",
    "    state = env.reset()\n",
    "    if render:\n",
    "        time.sleep(1.0)\n",
    "        clear_output(wait=True)\n",
    "        env.render()\n",
    "        \n",
    "    done = False\n",
    "    samples = []\n",
    "    while not done:\n",
    "        old_state = state\n",
    "        \n",
    "        # use the policy to get the next action\n",
    "        action = ...\n",
    "        \n",
    "        # apply the action in the environment\n",
    "        state, reward, done = env.step(action)\n",
    "        \n",
    "        samples.append((old_state, action, state, reward))\n",
    "        if render:\n",
    "            time.sleep(0.3)\n",
    "            clear_output(wait=True)\n",
    "            env.render()\n",
    "            \n",
    "    return samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q1a\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets watch the random policy in action:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(20)\n",
    "samples = run_policy(random_policy, simple_env, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "## Question 1b\n",
    "\n",
    "We just collected a dataset called `samples` which is a list of (state, action, next state, reward) tuples. For time step $t$, denote the collected tuple $(s_t, a_t, s'_t, r_t)$. We can now compute the discounted sum of rewards:\n",
    "\n",
    "$$\\sum_{t=1}^T \\gamma^{t-1} R(s_t, a_t, s'_t) $$\n",
    "\n",
    "Remember: this is what was inside the expectation in our new way of writing the value function! This sum is sometimes called the **return** of an episode. In the textbox below, given a list of $(s,a,s',r)$ tuples, you will calculate the discounted sum of rewards.\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q1b\n",
    "manual: false\n",
    "points: 1\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_return(samples, gamma=0.9):\n",
    "    \"\"\"Compute the discounted sum of rewards given a trajectory of samples.\n",
    "    \n",
    "    Inputs:\n",
    "        samples : list of tuple of float\n",
    "            A list of (state, action, next state, reward) tuples from running\n",
    "                the policy in the environment.\n",
    "        gamma : float\n",
    "            The discount factor.\n",
    "    \n",
    "    Returns:\n",
    "        value: float\n",
    "            The discounted sum of rewards from the samples list.\n",
    "    \"\"\"\n",
    "    \n",
    "    ...\n",
    "    \n",
    "    value = ...\n",
    "    return value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q1b\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The value function for the starting state is the **average** discounted sum of rewards given the randomness in the environment and the policy. Finally, we'll compute our Monte Carlo estimate. We collect several trajectories, calculate the return for each trajectory, and then take the sample average:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "returns = []\n",
    "np.random.seed(20)\n",
    "for _ in range(100):\n",
    "    samples = run_policy(random_policy, simple_env, False)\n",
    "    returns.append(calculate_return(samples))\n",
    "value = np.array(returns).mean()\n",
    "\n",
    "print(value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Monte Carlo policy evaluation gives us an estimate of the value of a single policy starting from a single state.  Your estimate above should be around $35$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "## Question 1c\n",
    "\n",
    "Before we move on, summarize in your own words what steps we take to compute a Monte Carlo estimate of the value function for a policy $\\pi$.\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q1c\n",
    "manual: true\n",
    "points: 1\n",
    "-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "\n",
    "\n",
    "The random policy spends a lot of time meandering so on average the value is much smaller than 100. By looking at the grid world diagram, you can see we could do better if the robot just moved to the right twice in a row and immediately reached the terminal state.\n",
    "\n",
    "Instead of evaluating a fixed policy, we can also use data to solve for the **optimal** policy. In the next section, we'll show you how to estimate the optimal policy using the Q-learning algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 2: Setting up Q-Learning \n",
    "\n",
    "Next, we will implement Q-learning for the grid world environment defined above. Recall that the optimal Q-function at a given state $s$ for an action $a$ is defined as\n",
    "$$Q(s, a) = \\sum_{s'} P(s'| a, s)\\left[R(s, a, s') + \\gamma \\max_{a'} Q(s', a')\\right]$$\n",
    "where $\\gamma$ is the discount factor, $P(s'| s,a)$ is the state transition probability function, and $R(s, a, s')$ is the reward function.\n",
    "\n",
    "**Note:** we can also define the $Q$-function as an average discounted sum like we did in Question 1. But for Q-learning, it's more convenient to use the recursive definition. This is a common trick in RL. You switch back and forth between the recursive and discounted sum definitions depending on what makes your problem easier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "## Question 2a: Getting the optimal policy from the optimal Q-function.\n",
    "\n",
    "We know that the optimal policy chooses the action which maximizes the optimal Q-function at a given state:\n",
    "\n",
    "$$\\pi^*(s) = \\text{argmax}_a Q(s, a)$$\n",
    "\n",
    "Fill in the following code which takes a state and a Q-function and then returns a list of the maximizing actions. **If there is more than one action with the highest Q-value estimate, include them all in the list.**\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q2a\n",
    "manual: false\n",
    "points: 1\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def greedy_action(state, Q_values):\n",
    "    \"\"\"Given a state and a Q-function, return the actions which maximizes the Q-function.\n",
    "    \n",
    "    Inputs:\n",
    "        state : int\n",
    "        Q_values : dict of dict\n",
    "            A Q function. The first index is over states\n",
    "            while the second index is over actions. So for example\n",
    "            Q_values[(1, 2)][\"north\"] is the Q-value for the state at position\n",
    "            (1, 2) and with action \"north\".\n",
    "            \n",
    "    Returns:\n",
    "        best_actions : list of string\n",
    "            The actions which maximizes the Q-function at the given state. \n",
    "    \"\"\"\n",
    "    \n",
    "    best_actions = []\n",
    "    best_value = -float(\"inf\")\n",
    "    for action, value in Q_values[state].items():\n",
    "        ...\n",
    "    \n",
    "    return best_actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q2a\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the cell below, we provide you with a simple GridWorld and it's optimal Q-function so that you can test your code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = GridWorld([[\"R\", \"100\"],\n",
    "                 [\"\", \"100\"],\n",
    "                 [\"100\", \"X\"]], stochastic=False)\n",
    "env.render()\n",
    "\n",
    "Qopt = { (0, 0): {'north': 90, 'west': 90,'east': 100,'south': 90},\n",
    "         (0, 1): {'north': 0, 'west': 0, 'east': 0,'south': 0},\n",
    "         (1, 0): {'north': 90, 'west': 90, 'east': 100,'south': 100},\n",
    "         (1, 1): {'north': 0, 'west': 0, 'east': 0, 'south': 0},\n",
    "         (2, 0): {'north': 0, 'west': 0, 'east': 0, 'south': 0},\n",
    "         (2, 1): {'north': 0, 'west': 0, 'east': 0, 'south': 0} }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "## Question 2b:  Learning the Q-function \n",
    "\n",
    "Using the code above, if we knew the optimal Q-function, then we could find the optimal policy. If we knew $P$ and $R$, then we could compute the optimal Q-function using Q iteration. But what if we don't know $P$ and $R$? There is a very clever algorithm called **Q-learning** which can solve for $Q(s,a)$ by collecting data. For Monte Carlo estimation, we needed to collect a bunch of trajectories all at once using a single policy. Q-learning is an online algorithm which **adapts** the data-collection policy after every action, getting closer and closer to the optimal policy.  \n",
    "\n",
    "After every step, we update our estimate of the optimal Q-function using the new state and reward we observe. For example, say we have some estimate of the Q-function $\\hat{Q}_k$ after observing $k$ samples, and say we observe a new sample which consists of $s$ the state we were at, $a$ the action we performed, $s'$ the state we ended up at, and $r$ the reward we got. Then our updated $Q$ function is given by\n",
    "$$\\hat{Q}_{k + 1}(s, a) \\leftarrow (1 - \\alpha)\\hat{Q}_k(s, a) + \\alpha \\left[r + \\gamma\\max_{a'} \\hat{Q}_k(s', a')\\right]$$\n",
    "where $\\alpha$ is a parameter between $0$ and $1$ that we set.\n",
    "\n",
    "### Fill in the function below, which updates the Q function using observed samples. \n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q2b\n",
    "manual: false\n",
    "points: 1\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_Q(Q_values, old_state, action, new_state, reward, gamma, alpha):\n",
    "    \"\"\"Given an old estimate of the Q function (given by the Q_values dict),\n",
    "    compute a new estimate by using observed samples.\n",
    "    \n",
    "    Modifies the Q_values dict in place, and does not return anything.\n",
    "    \n",
    "    Inputs:\n",
    "        Q_values : dict of dict\n",
    "            The estimate of the optimal Q values. The first index is over states\n",
    "            while the second index is over actions. So for example\n",
    "            Q_values[(1, 2)][\"north\"] is the Q-value for the state at position\n",
    "            (1, 2) and with action \"north\".\n",
    "        old_state : tuple of int\n",
    "            The state we were previously at before making the given action. The\n",
    "            first index represents the row while the second index represents the\n",
    "            column of the state.\n",
    "        action : string\n",
    "            The action we made. Can either be \"north\", \"east\", \"west\" or \"south\".\n",
    "        new_state : tuple of int\n",
    "            The state we transitioned to after making the given action.\n",
    "        reward : float\n",
    "            The reward we obtained after making our action.\n",
    "        gamma : float\n",
    "            The discount factor for the Q-function.\n",
    "        alpha : float\n",
    "            The proportion that tells us how we will weigh new incoming estimates of Q.\n",
    "    \"\"\"\n",
    "    # First compute the maximum Q-value at the new state.\n",
    "    max_Q = ...\n",
    "\n",
    "    # Now update the new Q value estimates in place.\n",
    "    ...\n",
    "    \n",
    "    return Q_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q2b\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is another cell for testing your code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = GridWorld([[\"R\", \"100\"],\n",
    "                 [\"\", \"100\"],\n",
    "                 [\"100\", \"X\"]], stochastic=False)\n",
    "env.render()\n",
    "\n",
    "Q_current = { (0, 0): {'north': 10, 'west': 15,'east': 20,'south': 10},\n",
    "              (0, 1): {'north': 0, 'west': 0, 'east': 0,'south': 0},\n",
    "              (1, 0): {'north': 20, 'west': 15, 'east': 10,'south': 20},\n",
    "              (1, 1): {'north': 0, 'west': 0, 'east': 0, 'south': 0},\n",
    "              (2, 0): {'north': 0, 'west': 0, 'east': 0, 'south': 0},\n",
    "              (2, 1): {'north': 0, 'west': 0, 'east': 0, 'south': 0} }\n",
    "\n",
    "old_state = (0,0)\n",
    "action = 'east'\n",
    "new_state = (0,1)\n",
    "reward = 100\n",
    "\n",
    "update_Q(copy.deepcopy(Q_current), old_state, action, new_state, reward, 0.75, 0.15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2c: Q-Learning Agent\n",
    "\n",
    "Now that we've implemented our Q-fuction update, we will define a agent that runs Q-learning in the environment. The agent will use two different exploration policies:\n",
    "\n",
    "1. greedy policy: The first type of agent always picks an action that maximizes our current estimate of the Q-function.\n",
    "2. $\\epsilon$-greedy policy: The second type picks the best estimate of the Q-function with probability $1-\\epsilon$, and picks a random action uniformly from the action space with probability $\\epsilon$. \n",
    "\n",
    "The first type is called a greedy agent while the second type is called an $\\epsilon$-greedy agent. Remember that taking greedy actions according to the **optimal** Q-function always results in the optimal policy. But when we are still learning the Q-function, we will see why $\\epsilon$-greedy agents are sometimes useful. \n",
    "\n",
    "First we define some useful helper functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_Q(env):\n",
    "    \"\"\"Return initial Q-value estimates with all values 0.\"\"\"\n",
    "    Q_values = {}\n",
    "    for i in range(env.num_rows):\n",
    "        for j in range(env.num_cols):\n",
    "            Q_values[i, j] = {\n",
    "                \"north\": 0.0,\n",
    "                \"west\": 0.0,\n",
    "                \"east\": 0.0,\n",
    "                \"south\": 0.0\n",
    "            }\n",
    "    return Q_values\n",
    "\n",
    "def visualize_value(Q, env):\n",
    "    for i, row in enumerate(env._grid):\n",
    "        row_strs = []\n",
    "        for j, elt in enumerate(row):\n",
    "            sys.stdout.write(\" -------\")\n",
    "            \n",
    "            action = greedy_action((i,j), Q)[0]\n",
    "            elt = \"{:.3f}\".format(Q[(i,j)][action]) \n",
    "            \n",
    "            row_strs.append(elt.center(7))\n",
    "        sys.stdout.write(\"\\n\")\n",
    "        sys.stdout.write(\"|\" + \"|\".join(row_strs) + \"|\")\n",
    "        sys.stdout.write(\"\\n\")\n",
    "    for _ in range(env.num_cols):\n",
    "        sys.stdout.write(\" -------\")\n",
    "    sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "### Fill in the `run_agent` function below that runs both of these types of agents.\n",
    "\n",
    "Note: Because of the randomness inherent in the function, you will have to search through the actions in the order \"north\", \"west\", \"east\", \"south\" when \n",
    "looking for which arms have the maximum value and you will have to use `np.random.choice` to choose.\n",
    "\n",
    "**Hint:** you already wrote code that returns a list of actions that maximize the Q-function.\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q2c\n",
    "manual: false\n",
    "points: 1\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: fill in the best actions for the agent in the run_agent function.\n",
    "# The run_agent function can run either the greedy agent or the epsilon-greedy agent, \n",
    "# depending on the parameter epsilon that gets passed in.\n",
    "    \n",
    "def run_agent(Q_values, env, num_rollouts, gamma=0.9, alpha=0.1, epsilon=0.0, render=False, show_train=False):\n",
    "    \"\"\"Run a Q-learning agent in a given environment.\n",
    "    \n",
    "    Inputs:\n",
    "        Q_values : dict of dict\n",
    "            The Q value estimates to start from.\n",
    "        env : GridWorld\n",
    "            The environment in which to run the agent.\n",
    "        num_rollouts : int\n",
    "            The number of times we wish to reset the environment\n",
    "            to its original state.\n",
    "        gamma : float\n",
    "            The discount factor for the Q-function.\n",
    "        alpha : float\n",
    "            The proportion that tells us how we will weigh new incoming estimates of Q.\n",
    "        epsilon : float\n",
    "            The proportion of times the agent will randomly pick an action instead\n",
    "            of making the optimal move in terms of the current estimate Q-function.\n",
    "            If epsilon is set to 0 this corresponds to a greedy agent.\n",
    "        render : bool\n",
    "            Whether to print the environment as it goes through each iteration.\n",
    "        show_train: bool\n",
    "            If render is True, then show_train will also visualize the estimated value function\n",
    "    \n",
    "    Returns:\n",
    "        Q_values : dict of dict\n",
    "            The learned Q values. The first index is over states\n",
    "            while the second index is over actions. So for example\n",
    "            Q_values[(1, 2)][\"north\"] is the Q-value for the state at position\n",
    "            (1, 2) and with action \"north\".\n",
    "    \"\"\"\n",
    "    for i in range(num_rollouts):\n",
    "        state = env.reset()\n",
    "        if render:\n",
    "            time.sleep(1.0)\n",
    "            clear_output(wait=True)\n",
    "            env.render()\n",
    "            print()\n",
    "            if show_train:\n",
    "                visualize_value(Q_values, env)\n",
    "        done = False\n",
    "        samples = []\n",
    "        while not done:\n",
    "            if np.random.binomial(1, epsilon):\n",
    "                action = random_policy(state) # take random action\n",
    "            else:\n",
    "                # Take the best action according to the Q-value estimate.\n",
    "                # If multiple values are equal, randomly chose between them.\n",
    "                best_actions = ...\n",
    "                action = ...\n",
    "            old_state = state\n",
    "            state, reward, done = env.step(action) # update state\n",
    "            samples.append((old_state, action, state, reward))\n",
    "            if render:\n",
    "                time.sleep(0.3)\n",
    "                clear_output(wait=True)\n",
    "                env.render()\n",
    "                print()\n",
    "                if show_train:\n",
    "                    visualize_value(Q_values, env)\n",
    "        # Update the Q-function using samples from this rollout.\n",
    "        for old_state, action, state, reward in reversed(samples):\n",
    "            Q_values = update_Q(Q_values, old_state, action, state, reward, gamma, alpha)\n",
    "    return Q_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q2c\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3. Q-learning in a deterministic setting \n",
    "Let's compare the behavior of the greedy agent vs. the $\\epsilon$-greedy agent. We consider a two-path setting where the agent can either receive a small reward by following a short path or a large reward by following a long path. In this setting, for any set of actions, the reward is deterministic.\n",
    "\n",
    "In the printed Grid World, R represents the agent, X's represent obstacles, and numbers represent rewards for reaching those positions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the world.\n",
    "env = GridWorld([[\"\",  \"\", \"\", \"\", \"\", \"100\",  \"X\", ],\n",
    "                 [\"\",  \"X\", \"X\", \"X\", \"X\", \"X\", \"X\"],\n",
    "                 [\"\",  \"\", \"\", \"\", \"R\", \"\",  \"1\"]], stochastic=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First, we'll watch the first 5 training iterations with actions the agent:\n",
    "\n",
    "Visualized underneath the grid is the current estimate of the value function (i.e. the estimated Q function after choosing the maximizing action in each state). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(4)\n",
    "Q_values = init_Q(env)\n",
    "Q_values = run_agent(Q_values, env, 5, alpha=0.5, render=True, show_train=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now we'll run training for 100 rollouts and then observe the behavior of the agent under the resulting Q estimate:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# No coding TODOs here, just run this cell to observe what the greedy agent does.\n",
    "np.random.seed(0)\n",
    "\n",
    "# Initialize the Q-value estimates.\n",
    "Q_values = init_Q(env)\n",
    "\n",
    "# TRAIN: Learn the Q-value for 100 rollouts.\n",
    "Q_values = run_agent(Q_values, env, 100, alpha=0.5, render=False)\n",
    "\n",
    "# Now let's see what the agent plays after learning the Q-values.\n",
    "Q_values = run_agent(Q_values, env, 1, render=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_value(Q_values, env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "## Question 3a: \n",
    "\n",
    "Which reward did the agent go for? Why do you think this happened? \n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q3a\n",
    "manual: true\n",
    "points: 1\n",
    "-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "\n",
    "\n",
    "### Now let's try to run a simple $\\epsilon$-greedy agent in this setting.\n",
    "\n",
    "Let's try setting $\\epsilon$ to 0.3 in this case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# No coding TODOs here, just run this cell to observe what the epsilon-greedy agent does.\n",
    "np.random.seed(0)\n",
    "\n",
    "# Initialize the Q-value estimates.\n",
    "Q_values = init_Q(env)\n",
    "\n",
    "# Learn the Q-value for 100 rollouts.\n",
    "Q_values = run_agent(Q_values, env, 100, epsilon=0.3, alpha=1.0, render=False)\n",
    "# Now let's see what the agent plays after learning the Q-values.\n",
    "Q_values = run_agent(Q_values, env, 1, render=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_value(Q_values, env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "### Question 3b: \n",
    "\n",
    "What did the $\\epsilon$-greedy agent do? If it was different from the greedy agent, why do you think this happened?\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q3b\n",
    "manual: true\n",
    "points: 1\n",
    "-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "\n",
    "\n",
    "## Question 4. Q-learning in a stochastic setting\n",
    "Finally, let's consider the stochastic setting. In stochastic Grid World, the agent moves in its chosen direction with probability 0.8, and moves in each direction orthogonal to the direction it had meant to go in with probability 0.1. We'll consider a setting where we have a bridge that leads to a high-value end-state. However, crossing carries with it a **risk** of falling down the side of the bridge.\n",
    "\n",
    "For this simulation, we will just look at an $\\epsilon$-greedy agent with $\\epsilon = 0.3$. This time, we will observe the effects of changing the the discount factor $\\gamma$ for the Q-function.\n",
    "\n",
    "### In the cell below, test some different values of $\\gamma$ and try to find a value of $\\gamma$ that will lead the agent to try to cross the bridge and a value of $\\gamma$ that will lead the agent to take the lower valued option.\n",
    "\n",
    "There aren't any special characters to mark the bridge here, the bridge is just represented by the two blank squares with -100's on either side. The agent has a small chance of falling into a squares with -100 reward when passing through each of these \"bridge\" squares. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: run this cell with different values of gamma.\n",
    "np.random.seed(0)\n",
    "\n",
    "# Initialize the world.\n",
    "env = GridWorld([[\"\",  \"\", \"\", \"\", \"\", \"\", \"\",  \"-100\", \"-100\", \"\"],\n",
    "                 [\"1\", \"\", \"\", \"\", \"\", \"R\", \"\",\"\", \"\",\"200\"],\n",
    "                 [\"\",  \"\", \"\", \"\", \"\", \"\", \"\",  \"-100\", \"-100\", \"\"]], stochastic=True)\n",
    "\n",
    "# Initialize the Q-value estimates.\n",
    "Q_values = init_Q(env)\n",
    "\n",
    "# Learn the Q-value for 1000 rollouts.\n",
    "# Try different values of gamma from 0.1 to 0.9\n",
    "Q_values = run_agent(Q_values, env, 1000, epsilon=0.3, gamma=0.9, alpha=0.1, render=False)\n",
    "\n",
    "# Now let's see what the agent plays after learning the Q-values.\n",
    "Q_values = run_agent(Q_values, env, 1, render=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_value(Q_values, env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "## Question 4a\n",
    "\n",
    "What values of gamma did you use to achieve each behavior? Why did these values work?\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q4a\n",
    "manual: true\n",
    "points: 1\n",
    "-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.image as mpimg\n",
    "img = mpimg.imread('qt.jpg')\n",
    "imgplot = plt.imshow(img)\n",
    "imgplot.axes.get_xaxis().set_visible(False)\n",
    "imgplot.axes.get_yaxis().set_visible(False)\n",
    "print(\"Yay, you've made it to the end of Lab 10!\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "---\n",
    "\n",
    "To double-check your work, the cell below will rerun all of the autograder tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check_all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "## Submission\n",
    "\n",
    "Make sure you have run all cells in your notebook in order before running the cell below, so that all images/graphs appear in the output. The cell below will generate a zip file for you to submit. **Please save before exporting!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Save your notebook first, then run this cell to export your submission.\n",
    "grader.export()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.12 ('m1_tf3')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "5aa7e46ca2034a31eb2def7e0666e2a45a309effbbb95578b9b862b165f42bf8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
