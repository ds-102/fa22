{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data 102 Fall 2022 Lecture 4: Binary Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "sns.set()  # This helps make our plots look nicer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_2x2_table(reality, decisions):\n",
    "    return pd.DataFrame(\n",
    "        confusion_matrix(reality, decisions),\n",
    "        columns = [\"D=0\", \"D=1\"],\n",
    "        index = [\"R=0\", \"R=1\"]\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decisions by thresholding: binary classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we'll work with a dataset predicting breast cancer from biopsy data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dataset = load_breast_cancer()\n",
    "X = pd.DataFrame(dataset['data'], columns = dataset['feature_names'])\n",
    "y = dataset['target']\n",
    "\n",
    "# Randomly flip 20% of the outputs to make the problem a little harder\n",
    "np.random.seed(42)\n",
    "mask = np.random.random(y.shape) < 0.2\n",
    "y[mask] = 1 - y[mask]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size = 0.33, random_state = 42\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use logistic regression to predict y from X:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LogisticRegression(solver = \"liblinear\")\n",
    "model.fit(X_train, y_train)\n",
    "y_hat_test = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can use our 2x2 table to evaluate the performance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_2x2_table(reality=y_test, decisions=y_hat_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Check: what is the false positive rate for these predictions? What about the false discovery rate?*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr = 32 / (32 + 41)\n",
    "fpr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fdr = 32 / (32 + 99)\n",
    "fdr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tpr = 99 / (99 + 16)\n",
    "tpr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For patients with cancer (R=1), we're correct $86\\%$ of the time. What if this isn't enough? Suppose we need a higher true positive rate: what can we do?\n",
    "\n",
    "Recall that a logistic regression model's predictions are probabilities between 0 and 1: we always threshold these to obtain binary decisions.\n",
    "\n",
    "So, let's look at the probabilities directly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "predicted_probs = model.predict_proba(X_test)[:, 1]\n",
    "predicted_probs.round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The predictions we used earlier were based on thresholding these probabilities at $0.5$. What if we try a different threshold?\n",
    "\n",
    "If we want to do better than $86\\%$ on patients with cancer, should the threshold be higher or lower than $0.5$? Experiment with different thresholds in this cell. What do you find?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 0.5\n",
    "y_preds = (predicted_probs > threshold).astype(int)\n",
    "make_2x2_table(y_test, y_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 0.2\n",
    "y_preds = (predicted_probs > threshold).astype(int)\n",
    "make_2x2_table(y_test, y_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 0.8\n",
    "y_preds = (predicted_probs > threshold).astype(int)\n",
    "make_2x2_table(y_test, y_preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize these predictions and our threshold:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 0.5\n",
    "\n",
    "plt.figure(figsize = (10,5))\n",
    "sns.stripplot(x=predicted_probs, y=y_test, alpha = 0.8, order = [0, 1], orient = \"h\")\n",
    "plt.axvline(threshold, c = \"k\", label = \"threshold\")\n",
    "plt.xlabel(\"Predicted class 1 probability\")\n",
    "plt.ylabel(\"True label\")\n",
    "plt.title(\"True class vs predicted class probability\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The blue points on the top correspond to healthy patients (R=0), and the orange points on the bototm correspond to cancer patients (R=1). For any point that falls to the left of the black line, we declare D=0.\n",
    "\n",
    "Using this plot, where do we need to set the threshold to guarantee all the cancer patients (blue points) are classified correctly?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ROC curves"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of trying one threshold at a time and seeing what we get, it would be nice if we could visualize the results from multiple thresholds all at once. This is what an ROC curve is for."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tpr_fpr(reality, decision_probs, threshold):\n",
    "    # Compute the denominators for the top and bottom rows (reality=0, reality=1)\n",
    "    r1_count = reality.sum()\n",
    "    r0_count = (1-reality).sum()\n",
    "    decisions = (decision_probs >= threshold).astype(int)\n",
    "    table = confusion_matrix(reality, decisions)\n",
    "    tp_count = table[1,1]\n",
    "    fp_count = table[0,1]\n",
    "    tpr = tp_count / r1_count\n",
    "    fpr = fp_count / r0_count\n",
    "    \n",
    "    return tpr, fpr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "thresholds = np.arange(0, 1.01, 0.1)\n",
    "for threshold in thresholds:\n",
    "    TPR, FPR = get_tpr_fpr(y_test, predicted_probs, threshold)\n",
    "    results.append((threshold, TPR, FPR))\n",
    "results = pd.DataFrame(results, columns = [\"threshold\", \"TPR\", \"FPR\"])\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(5, 5))\n",
    "plt.scatter(results[\"FPR\"], results[\"TPR\"])\n",
    "plt.xlabel(\"FPR\")\n",
    "plt.ylabel(\"TPR\")\n",
    "plt.xlim([-0.05, 1.05])\n",
    "plt.ylim([-0.05, 1.05])\n",
    "plt.title(\"FPR vs TPR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(5, 5))\n",
    "plt.plot(results[\"FPR\"], results[\"TPR\"])\n",
    "plt.xlabel(\"FPR\")\n",
    "plt.ylabel(\"TPR\")\n",
    "plt.xlim([-0.05, 1.05])\n",
    "plt.ylim([-0.05, 1.05])\n",
    "plt.title(\"FPR vs TPR\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ROC curves in scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`scikit-learn` already does (almost) all this work for us, using the `roc_curve` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr, tpr, _ = roc_curve(y_test, predicted_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(5, 5))\n",
    "plt.plot(fpr, tpr)\n",
    "plt.xlabel(\"FPR\")\n",
    "plt.ylabel(\"TPR\")\n",
    "plt.xlim([-0.05, 1.05])\n",
    "plt.ylim([-0.05, 1.05])\n",
    "plt.title(\"ROC curve (TPR vs FPR)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Precision-recall curves"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ROC curve is useful if we want to compare the tradeoff between doing well when reality = 0 and doing well when reality = 1 (in other words, between performance in the top row and bottom of our table).\n",
    "\n",
    "We can also look at the tradeoff between FDP (column-wise performance for the right column) and TPR (row-wise performance for the bottom row). The standard way that people do this is by plotting a precision-recall curve. The precision is defined to be 1 - FDP (in other words, when we make a discovery, how often is that discovery correct?)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision, recall, _ = precision_recall_curve(y_test, predicted_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize = (5, 5))\n",
    "plt.plot(recall, precision)\n",
    "plt.xlabel(\"Recall (TPR)\")\n",
    "plt.ylabel(\"Precision (1-FDP)\")\n",
    "plt.xlim([-0.05, 1.05])\n",
    "plt.ylim([-0.05, 1.05])\n",
    "plt.title(\"Precision (1-FDP) vs Recall (TPR)\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
